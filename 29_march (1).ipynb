{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bd6a2e88",
   "metadata": {},
   "source": [
    "# Assignment\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca8d9e39",
   "metadata": {},
   "source": [
    "1. What is Lasso Regression, and how does it differ from other regression techniques?\n",
    "\n",
    "Lasso regression is a statistical and machine learning method that performs variable selection and regularization. It's also known as the penalized regression method. Uses L1 regularization, which adds the absolute values of the coefficients multiplied by a regularization parameter to the ordinary least squares objective function.\n",
    "\n",
    "#### Lasso vs Ridge Regression:\n",
    "\n",
    "Ridge uses L2 regularization, adding the squared values of coefficients to the objective function.\n",
    "Lasso tends to produce sparse models, while Ridge tends to shrink coefficients towards zero without eliminating them.\n",
    "#### Lasso vs Ordinary Least Squares (OLS):\n",
    "\n",
    "OLS minimizes the sum of squared errors without any penalty term.\n",
    "\n",
    "Lasso includes a penalty term that can drive some coefficients to exactly zero, effectively performing feature selection.\n",
    "#### Lasso vs Elastic Net:\n",
    "\n",
    "Elastic Net combines L1 and L2 regularization.\n",
    "\n",
    "Lasso is a special case of Elastic Net when the L2 penalty is set to zero.\n",
    "#### Lasso vs Logistic Regression:\n",
    "\n",
    "Logistic Regression is used for binary classification problems.\n",
    "\n",
    "Lasso Regression is used for predicting a continuous outcome.\n",
    "#### Lasso vs Decision Trees/Random Forests:\n",
    "\n",
    "Lasso is a linear model and assumes a linear relationship between features and the target.\n",
    "Decision Trees/Random Forests can capture non-linear relationships."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f03320c",
   "metadata": {},
   "source": [
    "2. What is the main advantage of using Lasso Regression in feature selection?\n",
    "\n",
    "Lasso Regression can automatically perform feature selection by driving some coefficients to exactly zero, effectively identifying and excluding irrelevant or less important features from the model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27392490",
   "metadata": {},
   "source": [
    "3. How do you interpret the coefficients of a Lasso Regression model?\n",
    "\n",
    "In Lasso Regression, non-zero coefficients indicate the importance of corresponding features in predicting the target variable. A coefficient of zero suggests that the associated feature has been effectively excluded from the model, contributing to automatic feature selection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a03df4e5",
   "metadata": {},
   "source": [
    "4. What are the tuning parameters that can be adjusted in Lasso Regression, and how do they affect the model's performance?\n",
    "\n",
    "Tuning Parameters in Lasso Regression:\n",
    "\n",
    "##### Alpha (λ):\n",
    "\n",
    "Represents the strength of the regularization penalty.\n",
    "Controls the trade-off between fitting the data well and keeping the coefficients small.\n",
    "Larger values of alpha increase the penalty, leading to more coefficients being driven to zero.\n",
    "\n",
    "###### Effect :\n",
    "Alpha:\n",
    "\n",
    "Low Alpha: Closer to Ordinary Least Squares (OLS), may overfit the data.\n",
    "High Alpha: Increases sparsity, encourages feature selection, prevents overfitting.\n",
    "\n",
    "##### Max Iterations:\n",
    "\n",
    "Maximum number of iterations for the optimization algorithm to converge.\n",
    "May need adjustment if the model is not converging.\n",
    "\n",
    "Effech \n",
    "\n",
    "##### Max Iterations:\n",
    "\n",
    "Adequate iterations ensure convergence; too few may result in a suboptimal model.\n",
    "Adjust as needed for the algorithm to converge.\n",
    "\n",
    "Overall:\n",
    "\n",
    "Proper tuning balances model complexity and data fitting.\n",
    "Cross-validation helps find optimal values.\n",
    "Regularization (via alpha) controls overfitting, and max iterations ensure convergence."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf5f9fd",
   "metadata": {},
   "source": [
    "5. Can Lasso Regression be used for non-linear regression problems? If yes, how?\n",
    "\n",
    "Lasso Regression is inherently a linear regression technique and assumes a linear relationship between the features and the target variable. Therefore, it is not directly suitable for handling non-linear regression problems. However, there are ways to adapt Lasso Regression for non-linear relationships:\n",
    "\n",
    "Feature Transformation:\n",
    "\n",
    "Transform the original features into higher-order polynomial features to capture non-linear relationships. For example, introduce squared, cubed, or other polynomial terms.\n",
    "Interaction Terms:\n",
    "\n",
    "Include interaction terms between different features to account for non-linear interactions.\n",
    "Kernel Methods:\n",
    "\n",
    "Use kernelized versions of Lasso Regression, such as the kernelized Lasso, which applies the kernel trick to map the features into a higher-dimensional space, allowing the model to capture non-linear patterns.\n",
    "Ensemble Methods:\n",
    "\n",
    "Combine multiple Lasso models or other regression models using ensemble techniques like Random Forests or Gradient Boosting. These methods can capture non-linear relationships by aggregating the predictions of multiple weak models.\n",
    "Non-linear Lasso Extensions:\n",
    "\n",
    "Explore extensions of Lasso Regression designed for non-linear relationships, such as the Elastic Net, which combines L1 and L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4e1e2f8",
   "metadata": {},
   "source": [
    "6. What is the difference between Ridge Regression and Lasso Regression?\n",
    "\n",
    "Ridge Regression vs. Lasso Regression:\n",
    "\n",
    "Regularization Type:\n",
    "\n",
    "Ridge Regression: L2 regularization, adds the squared values of coefficients.\n",
    "Lasso Regression: L1 regularization, adds the absolute values of coefficients.\n",
    "Penalty Term Impact:\n",
    "\n",
    "Ridge: Tends to shrink all coefficients towards zero, but rarely sets them exactly to zero.\n",
    "Lasso: Encourages sparsity, setting some coefficients exactly to zero, leading to automatic feature selection.\n",
    "Feature Selection:\n",
    "\n",
    "Ridge: Does not perform automatic feature selection.\n",
    "Lasso: Can automatically exclude irrelevant features by driving some coefficients to zero.\n",
    "Solution Stability:\n",
    "\n",
    "Ridge: More stable when there is multicollinearity among features.\n",
    "Lasso: May arbitrarily select one variable over another in the presence of highly correlated features.\n",
    "Use Cases:\n",
    "\n",
    "Ridge: Suitable when all features are potentially relevant.\n",
    "Lasso: Useful when there's a suspicion that many features are irrelevant or redundant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0276ca0",
   "metadata": {},
   "source": [
    "7. Can Lasso Regression handle multicollinearity in the input features? If yes, how?\n",
    "\n",
    "\n",
    "Handling Multicollinearity in Lasso Regression:\n",
    "\n",
    "Variable Selection:\n",
    "\n",
    "Lasso's L1 regularization encourages sparsity, leading to automatic feature selection.\n",
    "In the presence of multicollinearity, Lasso tends to choose one variable over others and sets coefficients of less important variables to zero.\n",
    "Automatic Feature Elimination:\n",
    "\n",
    "By setting certain coefficients to zero, Lasso effectively eliminates some features from the model.\n",
    "This can mitigate multicollinearity issues by focusing on the most relevant features.\n",
    "Sparse Solutions:\n",
    "\n",
    "Lasso's ability to produce sparse solutions makes it suitable for situations where only a subset of features is truly influential.\n",
    "This sparsity helps in dealing with multicollinearity by emphasizing the importance of selected features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21383cc",
   "metadata": {},
   "source": [
    "8. How do you choose the optimal value of the regularization parameter (lambda) in Lasso Regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20eb7953",
   "metadata": {},
   "source": [
    "Choosing the Optimal Regularization Parameter (λ) in Lasso Regression:\n",
    "\n",
    "Cross-Validation:\n",
    "\n",
    "Perform k-fold cross-validation (e.g., 5-fold or 10-fold) on your training data.\n",
    "Vary λ and evaluate model performance for each fold.\n",
    "\n",
    "Grid Search:\n",
    "\n",
    "Define a range of λ values to test using a grid search approach.\n",
    "Evaluate the model's performance for each λ in the specified range.\n",
    "\n",
    "Select Optimal λ:\n",
    "\n",
    "Choose the λ that results in the best average performance across all folds or a specific metric (e.g., Mean Squared Error).\n",
    "Alternatively, use techniques like cross-validated Ridge Regression or Elastic Net, which combine Lasso and Ridge, offering a middle ground.\n",
    "\n",
    "Regularization Path:\n",
    "\n",
    "Visualize the regularization path, showing how coefficients change with different λ values.\n",
    "Identify the λ where some coefficients become exactly zero.\n",
    "\n",
    "Information Criteria:\n",
    "\n",
    "Use information criteria like AIC (Akaike Information Criterion) or BIC (Bayesian Information Criterion) to guide the selection of λ.\n",
    "These criteria balance model fit and complexity.\n",
    "\n",
    "Validation Set:\n",
    "\n",
    "Set aside a separate validation set to assess the model's performance with the chosen λ before applying it to the test set.\n",
    "\n",
    "Automated Methods:\n",
    "\n",
    "Explore automated methods like coordinate gradient descent, which adaptively adjusts λ during the optimization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "015827e0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a8884e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
